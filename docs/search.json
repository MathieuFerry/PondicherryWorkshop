[
  {
    "objectID": "Index.html",
    "href": "Index.html",
    "title": "Pondicherry Winter School W3 workshop (2023)",
    "section": "",
    "text": "Presentation\nThis page gathers the statistical textual analysis tutorials for the W3 workshop of the 2023 Pondicherry Winter School. The workshop will be led by Anupam Das, Aasim Khan and myself. I focus on exploratory textual statistical analysis.\nThough the workshop uses R, students do not need prior knowledge of this programming environment to go through the workshop, since we use two wonderful R shiny apps to analyse the data:\n\nRadiant developed by Vincent Nijs for descriptive statistics (cross tabulations)\nRainette developed by Julien Barnier for textual analysis based on Max Reinert’s method\n\nI also (quickly) introduce to textual data cleaning and pre-treatment based on the quanteda package (developed by Kenneth Benoit and Kohei Watanabe).\nIn this workshop, we work on an excerpt of a database web scraped from an Indian online matrimonial website. We analyse a few of the characteristics of the advertisements, by looking at how spouse-to-be present themselves. In particular, we are interested in how they described their family and what kind of partner they say they desire.\nI draw from a work conducted with Jeanne Subtil. You can read our working paper here.\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDiscovering the data\n\n\n\n\n\n\n\nExploratory textual analysis\n\n\n\n\n\n\n\nPreliminary tasks for W3\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Preliminary.html",
    "href": "Preliminary.html",
    "title": "Preliminary tasks for W3",
    "section": "",
    "text": "Welcome to the computational/quantitative/mixed methods workshop!\nAs part of this workshop, we (Anupam Das & I) are going to work on textual analysis. In my case, I will focus on automatic exploratory statistical tools. To this end, we will build from a research work I have been conducting with Jeanne Subtil (Sciences Po/CRIS) on online matrimonial advertisements in India. We explored online marriage-making strategies of Uttar Pradesh-based Hindu spouse-to-be individuals registered on the Jeevansathi platform. Our working paper is currently available here.\nBefore the workshop, I request you to complete two tasks:\n\n\nThe paper explains our research questions, how we constructed our database and how we conducted our analysis. We will expand on this research during the workshop and we will analyse a subset of the data. It is important to read the paper beforehand so as to have a better understanding of what we are working on.\n\n\n\n\n\n\n\nThis involves two points:\n\nDownload the sample database used during the workshop (available on our Google Drive or from the following links). There are two files where our data are stored: “sample.csv” and “sample.rdata”. These are exactly the same files except that the “sample.rdata” is a format native to R (the software we use) and allows to save more pre-formatting characteristics of the data so it will be this one we will use in our session, but you can start to have a look a it by opening the csv file.\nDownload the “TextualAnalysis.R” file. It contains the R script we will use during the workshop. If you are not familiar with programming and/or R language, don’t worry! Though we will need these lines of code, we will not spend time actually writing code lines during the workshop and we will rather play with apps (“shiny apps”) that are more user-friendly.\n\nIf you are already an R user and you are familiar with importing data in RStudio from your computer, feel free to use RStudio from your computer during the workshop and ignore the following instructions.\nIf you are not, do not worry! To avoid any technological complications, we are going to use R/RStudio from a web server for which you need to create an account. Note that you don’t need to know R to follow this workshop and this workshop is not a R workshop. Still, if you are curious about R, you can check this webpage which has a lot of resources: RStudio Education.\nFor our workshop, follow the following steps:\n\nGo to the Posit server website and click on “sign up”:\n\n\nThere, you can choose between several “plans”. Unless you are willing to spend money, choose the “free plan” and set up an account with your email ID.\n\nAfter that is done, you should be able to access the cloud from where we will conduct our analysis. Click on “New project” and choose “New RStudio Project”:\n\n\nYou can entitle this project as “Pondicherry workshop” if you wish.\n\nIf everything has worked fine, you are now in the RStudio cloud. The left panel is the “console”: we run commands from there to ask the software to conduct our analyses. The top right panel is your “Global environment”, that is where the data will appear once you have uploaded it in RStudio. The bottom right panel is your “File explorer” on the cloud.\nYou now need to upload the data (“sample.rdata”) and the R script (“TextualAnalysis.R”) to the cloud. To this end, click on the “Upload” button and select the files from your computer. Files need to be added one by one.\n\n\n\nOnce that is done, you can click on your two loaded files to open the R script and load the database into the R environment. Once that is done, you should see something like this:\n\n\nWe are done for now! We will start from there in our workshop sessions. If you have any issue running any of these steps, please contact me at mathieukferry@gmail.com."
  },
  {
    "objectID": "Preliminary.html#read-the-working-paper",
    "href": "Preliminary.html#read-the-working-paper",
    "title": "Preliminary tasks for W3",
    "section": "1. Read the working paper",
    "text": "1. Read the working paper\nThe paper explains our research questions, how we constructed our database and how we conducted our analysis. We will expand on this research during the workshop and we will analyse a subset of the data. It is important to read the paper beforehand so as to have a better understanding of what we are working on."
  },
  {
    "objectID": "Preliminary.html#get-ready-to-conduct-statistical-textual-analysis-during-the-workshop",
    "href": "Preliminary.html#get-ready-to-conduct-statistical-textual-analysis-during-the-workshop",
    "title": "Preliminary tasks for W3",
    "section": "2. Get ready to conduct statistical textual analysis during the workshop",
    "text": "2. Get ready to conduct statistical textual analysis during the workshop\nThis involves two points:\n\nDownload the sample database used during the workshop (available on our Google Drive or from the following links). There are two files where our data are stored: “sample.csv” and “sample.rdata”. These are exactly the same files except that the “sample.rdata” is a format native to R (the software we use) and allows to save more pre-formatting characteristics of the data so it will be this one we will use in our session, but you can start to have a look a it by opening the csv file.\nDownload the “TextualAnalysis.R” file. It contains the R script we will use during the workshop. If you are not familiar with programming and/or R language, don’t worry! Though we will need these lines of code, we will not spend time actually writing code lines during the workshop and we will rather play with apps (“shiny apps”) that are more user-friendly.\n\nIf you are already an R user and you are familiar with importing data in RStudio from your computer, feel free to use RStudio from your computer during the workshop and ignore the following instructions.\nIf you are not, do not worry! To avoid any technological complications, we are going to use R/RStudio from a web server for which you need to create an account. Note that you don’t need to know R to follow this workshop and this workshop is not a R workshop. Still, if you are curious about R, you can check this webpage which has a lot of resources: RStudio Education.\nFor our workshop, follow the following steps:\n\nGo to the Posit server website and click on “sign up”:\n\n\nThere, you can choose between several “plans”. Unless you are willing to spend money, choose the “free plan” and set up an account with your email ID.\n\nAfter that is done, you should be able to access the cloud from where we will conduct our analysis. Click on “New project” and choose “New RStudio Project”:\n\n\nYou can entitle this project as “Pondicherry workshop” if you wish.\n\nIf everything has worked fine, you are now in the RStudio cloud. The left panel is the “console”: we run commands from there to ask the software to conduct our analyses. The top right panel is your “Global environment”, that is where the data will appear once you have uploaded it in RStudio. The bottom right panel is your “File explorer” on the cloud.\nYou now need to upload the data (“sample.rdata”) and the R script (“TextualAnalysis.R”) to the cloud. To this end, click on the “Upload” button and select the files from your computer. Files need to be added one by one.\n\n\n\nOnce that is done, you can click on your two loaded files to open the R script and load the database into the R environment. Once that is done, you should see something like this:\n\n\nWe are done for now! We will start from there in our workshop sessions. If you have any issue running any of these steps, please contact me at mathieukferry@gmail.com."
  },
  {
    "objectID": "Discovering.html",
    "href": "Discovering.html",
    "title": "Discovering the data",
    "section": "",
    "text": "In this tutorial, we are first going to prepare R for the upcoming statistical analysis by downloading and loading necessary R packages into our environment. Packages are collections of functions that allow to use different statistical tools depending on your needs.\nHere, I am requesting you to download six packages. “dplyr” is a useful data management package, “radiant” opens a “shiny app” that allows to run statistical analyses without coding, “quanteda” and its two dependencies “quanteda.textstats” and “quanteda.textstats” are used for pre-treating text as data (it also contains a lot of textual analysis tools which we are not using here) and finally “rainette” is a “shiny app” to conduct exploratory textual analysis using the Reinert’s method.\n\n##For this workshop, we will need the following packages that you first need to install by running these commands:\ninstall.packages(\"dplyr\",repos = \"http://cran.us.r-project.org\") #for data management if needed\ninstall.packages(\"radiant\",repos = \"http://cran.us.r-project.org\") #for descriptive statistics\ninstall.packages(\"quanteda\", repos = \"http://cran.us.r-project.org\") #for textual data pre-treatment and basic analysis\ninstall.packages(\"quanteda.textstats\",repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"quanteda.textstats\",repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"rainette\",repos = \"http://cran.us.r-project.org\") #for exploratory textual analysis\n\nTo run the above R code lines, highlight them with your mouse cursor and then click on “run” (or type ctrl+Enter on the keyboard). There should be a lot of red lines appearing on the console but after a while this should stop and you should be fine…\nThen you can just load these packages into your R environment by running the below lines.\n\n##Then we can just load the packages\nlibrary(dplyr)\nlibrary(radiant)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(rainette)\n\nAfter that, we are going to load the database. If you are on the Posit server, you can just run the below lines to load the “sample.rdata” into your environment.\n\nsetwd(\"/cloud/project\")\nload(\"sample.rdata\")\n\n\n\n\nYou can have a look at the data by running the following code:\n\nView(sample)\n\n\nThe data contains 1,000 rows (1,000 matrimonial advertisements) and 11 columns (11 variables, corresponding to 11 different types of information collected from the matrimonial advertisement).\nThis database is an excerpt from the web-harvested matrimonial advertisement profiles that we collected for the “Matchmaking digitized” paper. If you remember, we harvested all Hindu male and female profiles based in Uttar Pradesh. Here we randomly selected 1,000 profiles from this list by adding the following condition: no missing variable on the selected 11 types of information. Missing information is an important feature is web-harvested profiles on network platforms as we talked about in the lecture. For simplicity, we do not engage here on this issue.\nNow, let’s inspect the data more closely by using the radiant shiny app.\n\nradiant::radiant_window()\n\nYou should see something like the picture below appearing. You again need to load the “sample.rdata” database by clicking on “Load” as indicated on the picture.\n\nThis gives you another preview of the database.\n\n\n\nLet’s have a more detailed description of the database. Of the 11 variables,\n\n7 are “factor” variables, i.e. they are categorical variables\n\nSex: corresponding to the gender of the spouse-to-be\nAge: corresponding to the age of the spouse-to-be\nEducation: highest degree of the spouse-to-be\nCaste: declared caste recoded as Brahmin, other upper caste, OBC, Dalit\nFamily_income: declared total family income\nFamily: family type (joint or nuclear)\nWho_runs_page: manager of the advertisement profile (Self if spouse-to-be, parent, sibling or other, including matrimonial agency)\n\n\nNote that all these variables have already been cleaned and recoded from the matrimonial advertisements (see the slides of the lecture where an example of a profile is provided).\nWe use the “Visualize” tab to have a quick plot describing our dataset based on these features. Play around the features of the tab to get a “nice” figure! Below is a suggestion.\nThanks to this, we are to see in a snapshot that we have much more male rather than female advertisement profiles, that the modal age category is 28 to 31, most of the profiles are at least graduate, upper caste, come from nuclear-type families which earn more than 17.5 lakh a year. Quite a selected sample compared to the Hindu population in Uttar Pradesh! Finally, though about half of the advertisements are managed by the spouse-to-be themselves, parents and siblings are also often involved.\n\nWho runs the page is very much associated to the gender of the spouse-to-be. We can use the “Pivot” tab to create a cross-table with row percentages (gender is the “independent” variable and is represented in the rows and profile management is the column variable as it is here the “dependent” variable):\n\nWe can do other explorations. For instance, does age vary depending on gender? As can be seen below, women tend to be on the whole younger than men on this matrimonial advertisement platform, possibly reflecting gender-specific age norms in marriage practices.\n\n\n\n\nThe 8th and 9th variables, called Desc_family and Desc_desiredpartner consists in “character” variable, i.e. they consist in text strings on the matrimonial ads where the family of the spouse-to-be is described and where the characteristics of the desired partner are indicated. We will look more closely on how to analyse these textual variables in the next session so we skip them for now.\nThe last two variables of the dataframe are nwords_family and nwords_despartner. I created “numeric” variables where I counted the number of words used to describe one’s family and one’s desired partner for each matrimonial advertisement.\nLet’s have a look at how these data looks like, first from the “Visualization” tab.\nThe histograms of the distributions for these two distributions suggest that we have two “skewed” distributions: most of the people write less than 50 words, but a few seem to write longer descriptions, so that the “tail” of the distribution expands a lot on the right.\nNotice also how the mode for desired partner is at a lower value, suggesting that matrimonial advertisements write more on who their family are, rather than what kind of partner they are aspiring to…\n\nYou can go to the “Explore” and analyse univariate descriptive statistics for these two numeric variables. Indeed, the mean, median and mode indicate that there is more written on family than desired partners on matrimonial advertisements!\nThe gap between the mean and the median points to the asymmetry of the distributions that we already identified in the histogram plot (regarding family and family description, as a “centrality” indicator, the mean is higher than the median because it is an indicator that is sensitive to outliers, and here the outliers are those who write a lot!)\nSD here means “standard deviation” and it is an indicator of the dispersion of the distributions. As the standard deviation of the number of words to describe one’s family is higher than for the desired partner, it suggests a higher dispersion of the length of family descriptions (there are both very short and very long family descriptions whereas for the desired partner descriptions, it is more centered). This idea is also quite well visualized on the preceding figure.\n\nWe can also explore whether the length of the family and desired partner descriptions vary according to the gender of the spouse-to-be and to who manages the advertisement profile.\nWe can do this exploration in the “Explore” tab to obtain tables or in the “Visualize” tab to obtain nice plots. Here, we clearly see that the length of the family description clearly depends on who manages the profile: when it is a parent or a sibling, it is on average longer than when the profile is managed by the spouse-to-be.\nRegarding the length of descriptions of the desired partner, they are on average slightly for female profiles (aspiring a male partner) than the reverse. Variations depending on who manages the profile seem less salient.\n\nCan you think of other factors that may affect how much people write about their family and about their desired partner? Education, socioeconomic level, age… try to think about mechanisms, and explore them with the data!\n\n\n\nIn this tutorial, we have explored our sample database of matrimonial advertisements. In so doing, we have described different types of variables (categorical and numerical) and how we can use cross-tables to analyse how one variable is associated to the other. Plots are a nice way to convey information.\nRadiant is a relatively easy app to handle and explore data.\nNote that we have not entered the fascinating worlds of inferential statistics (statistical tests) and multivariate analysis (regressions) here but the app offers these possibilities. Feel free to explore the many possible analyses!"
  },
  {
    "objectID": "Discovering.html#preparation-of-the-analysis",
    "href": "Discovering.html#preparation-of-the-analysis",
    "title": "Discovering the data",
    "section": "Preparation of the analysis",
    "text": "Preparation of the analysis\nIn this tutorial, we are first going to prepare R for the upcoming statistical analysis by downloading and loading necessary R packages into our environment. Packages are collections of functions that allow to use different statistical tools depending on your needs.\nHere, I am requesting you to download six packages. “dplyr” is a useful data management package, “radiant” opens a “shiny app” that allows to run statistical analyses without coding, “quanteda” and its two dependencies “quanteda.textstats” and “quanteda.textstats” are used for pre-treating text as data (it also contains a lot of textual analysis tools which we are not using here) and finally “rainette” is a “shiny app” to conduct exploratory textual analysis using the Reinert’s method.\n\n##For this workshop, we will need the following packages that you first need to install by running these commands:\ninstall.packages(\"dplyr\",repos = \"http://cran.us.r-project.org\") #for data management if needed\ninstall.packages(\"radiant\",repos = \"http://cran.us.r-project.org\") #for descriptive statistics\ninstall.packages(\"quanteda\", repos = \"http://cran.us.r-project.org\") #for textual data pre-treatment and basic analysis\ninstall.packages(\"quanteda.textstats\",repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"quanteda.textstats\",repos = \"http://cran.us.r-project.org\")\ninstall.packages(\"rainette\",repos = \"http://cran.us.r-project.org\") #for exploratory textual analysis\n\nTo run the above R code lines, highlight them with your mouse cursor and then click on “run” (or type ctrl+Enter on the keyboard). There should be a lot of red lines appearing on the console but after a while this should stop and you should be fine…\nThen you can just load these packages into your R environment by running the below lines.\n\n##Then we can just load the packages\nlibrary(dplyr)\nlibrary(radiant)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(rainette)\n\nAfter that, we are going to load the database. If you are on the Posit server, you can just run the below lines to load the “sample.rdata” into your environment.\n\nsetwd(\"/cloud/project\")\nload(\"sample.rdata\")"
  },
  {
    "objectID": "Discovering.html#inspecting-the-data",
    "href": "Discovering.html#inspecting-the-data",
    "title": "Discovering the data",
    "section": "Inspecting the data",
    "text": "Inspecting the data\nYou can have a look at the data by running the following code:\n\nView(sample)\n\n\nThe data contains 1,000 rows (1,000 matrimonial advertisements) and 11 columns (11 variables, corresponding to 11 different types of information collected from the matrimonial advertisement).\nThis database is an excerpt from the web-harvested matrimonial advertisement profiles that we collected for the “Matchmaking digitized” paper. If you remember, we harvested all Hindu male and female profiles based in Uttar Pradesh. Here we randomly selected 1,000 profiles from this list by adding the following condition: no missing variable on the selected 11 types of information. Missing information is an important feature is web-harvested profiles on network platforms as we talked about in the lecture. For simplicity, we do not engage here on this issue.\nNow, let’s inspect the data more closely by using the radiant shiny app.\n\nradiant::radiant_window()\n\nYou should see something like the picture below appearing. You again need to load the “sample.rdata” database by clicking on “Load” as indicated on the picture.\n\nThis gives you another preview of the database."
  },
  {
    "objectID": "Discovering.html#exploring-categorical-variables",
    "href": "Discovering.html#exploring-categorical-variables",
    "title": "Discovering the data",
    "section": "Exploring categorical variables",
    "text": "Exploring categorical variables\nLet’s have a more detailed description of the database. Of the 11 variables,\n\n7 are “factor” variables, i.e. they are categorical variables\n\nSex: corresponding to the gender of the spouse-to-be\nAge: corresponding to the age of the spouse-to-be\nEducation: highest degree of the spouse-to-be\nCaste: declared caste recoded as Brahmin, other upper caste, OBC, Dalit\nFamily_income: declared total family income\nFamily: family type (joint or nuclear)\nWho_runs_page: manager of the advertisement profile (Self if spouse-to-be, parent, sibling or other, including matrimonial agency)\n\n\nNote that all these variables have already been cleaned and recoded from the matrimonial advertisements (see the slides of the lecture where an example of a profile is provided).\nWe use the “Visualize” tab to have a quick plot describing our dataset based on these features. Play around the features of the tab to get a “nice” figure! Below is a suggestion.\nThanks to this, we are to see in a snapshot that we have much more male rather than female advertisement profiles, that the modal age category is 28 to 31, most of the profiles are at least graduate, upper caste, come from nuclear-type families which earn more than 17.5 lakh a year. Quite a selected sample compared to the Hindu population in Uttar Pradesh! Finally, though about half of the advertisements are managed by the spouse-to-be themselves, parents and siblings are also often involved.\n\nWho runs the page is very much associated to the gender of the spouse-to-be. We can use the “Pivot” tab to create a cross-table with row percentages (gender is the “independent” variable and is represented in the rows and profile management is the column variable as it is here the “dependent” variable):\n\nWe can do other explorations. For instance, does age vary depending on gender? As can be seen below, women tend to be on the whole younger than men on this matrimonial advertisement platform, possibly reflecting gender-specific age norms in marriage practices."
  },
  {
    "objectID": "Discovering.html#exploring-numeric-variables",
    "href": "Discovering.html#exploring-numeric-variables",
    "title": "Discovering the data",
    "section": "Exploring numeric variables",
    "text": "Exploring numeric variables\nThe 8th and 9th variables, called Desc_family and Desc_desiredpartner consists in “character” variable, i.e. they consist in text strings on the matrimonial ads where the family of the spouse-to-be is described and where the characteristics of the desired partner are indicated. We will look more closely on how to analyse these textual variables in the next session so we skip them for now.\nThe last two variables of the dataframe are nwords_family and nwords_despartner. I created “numeric” variables where I counted the number of words used to describe one’s family and one’s desired partner for each matrimonial advertisement.\nLet’s have a look at how these data looks like, first from the “Visualization” tab.\nThe histograms of the distributions for these two distributions suggest that we have two “skewed” distributions: most of the people write less than 50 words, but a few seem to write longer descriptions, so that the “tail” of the distribution expands a lot on the right.\nNotice also how the mode for desired partner is at a lower value, suggesting that matrimonial advertisements write more on who their family are, rather than what kind of partner they are aspiring to…\n\nYou can go to the “Explore” and analyse univariate descriptive statistics for these two numeric variables. Indeed, the mean, median and mode indicate that there is more written on family than desired partners on matrimonial advertisements!\nThe gap between the mean and the median points to the asymmetry of the distributions that we already identified in the histogram plot (regarding family and family description, as a “centrality” indicator, the mean is higher than the median because it is an indicator that is sensitive to outliers, and here the outliers are those who write a lot!)\nSD here means “standard deviation” and it is an indicator of the dispersion of the distributions. As the standard deviation of the number of words to describe one’s family is higher than for the desired partner, it suggests a higher dispersion of the length of family descriptions (there are both very short and very long family descriptions whereas for the desired partner descriptions, it is more centered). This idea is also quite well visualized on the preceding figure.\n\nWe can also explore whether the length of the family and desired partner descriptions vary according to the gender of the spouse-to-be and to who manages the advertisement profile.\nWe can do this exploration in the “Explore” tab to obtain tables or in the “Visualize” tab to obtain nice plots. Here, we clearly see that the length of the family description clearly depends on who manages the profile: when it is a parent or a sibling, it is on average longer than when the profile is managed by the spouse-to-be.\nRegarding the length of descriptions of the desired partner, they are on average slightly for female profiles (aspiring a male partner) than the reverse. Variations depending on who manages the profile seem less salient.\n\nCan you think of other factors that may affect how much people write about their family and about their desired partner? Education, socioeconomic level, age… try to think about mechanisms, and explore them with the data!"
  },
  {
    "objectID": "Discovering.html#to-conclude",
    "href": "Discovering.html#to-conclude",
    "title": "Discovering the data",
    "section": "To conclude",
    "text": "To conclude\nIn this tutorial, we have explored our sample database of matrimonial advertisements. In so doing, we have described different types of variables (categorical and numerical) and how we can use cross-tables to analyse how one variable is associated to the other. Plots are a nice way to convey information.\nRadiant is a relatively easy app to handle and explore data.\nNote that we have not entered the fascinating worlds of inferential statistics (statistical tests) and multivariate analysis (regressions) here but the app offers these possibilities. Feel free to explore the many possible analyses!"
  },
  {
    "objectID": "ExploratoryTextualAnalysis.html#family-descriptions-text-pre-treatment",
    "href": "ExploratoryTextualAnalysis.html#family-descriptions-text-pre-treatment",
    "title": "Exploratory textual analysis",
    "section": "Family descriptions: text pre-treatment",
    "text": "Family descriptions: text pre-treatment\nLet’s first load our packages and our dataset.\n\n##Then we can just load the packages\nlibrary(dplyr)\nlibrary(radiant)\nlibrary(quanteda)\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(rainette)\n\n\nsetwd(\"/cloud/project\")\nload(\"sample.rdata\")\n\nThe first step is to transform our “text” variable in the sample database into a specific “quanteda” object called corpus:\n\n##Transform to corpus\ncorpus &lt;- corpus(sample$Desc_family)\n\nThe corpus is a bundle of texts, here the 1000 different family descriptions on each different ads.\nBased on this object, we can then create “tokens,” corresponding to the words of the texts. In this process, we clean up a bit our object for the analysis:\n\nWe remove punctuation (dots, commas, etc) and symbols (e.g. * $ €…) so that we do not risk considering them as words\nWe also decided to remove numbers written as figures (and not in letters). A different choice could have been made but I think it’s ok to remove them.\n\n\n#Remove punctuation & symbols\ntok &lt;- tokens(corpus, remove_punct = TRUE,\n              remove_symbols=TRUE,\n              remove_numbers =TRUE)\n\nWe also specifically remove a number of “stop words” from the analysis. These all correspond to very common words/expressions in English. We consider that they do not help characterizing the diversity of family descriptions here so we remove them from the bundle of words. We have also removed a number of very common words in these descriptions that are not very informative, relating to the description of the family members. The argument is that we are interested in what is said about the family members but not in qualifying their presence or not.\n\nstopwords(\"en\")\n\n  [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n  [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\"    \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\"      \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\"      \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\"     \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\"      \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\"   \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\"    \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\"    \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\"     \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\"    \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\"    \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\"        \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\"        \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\"        \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\"     \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\"    \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\"        \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\"       \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\"     \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\"     \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\"       \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\"      \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\"        \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\"      \n[171] \"so\"         \"than\"       \"too\"        \"very\"       \"will\"      \n\n#Remove stop words & words relating to specific family members\ntok &lt;- tokens_remove(tok,c(stopwords(\"en\"),\"mother\",\"father\",\"mom\",\"dad\",\"sister\",\"sisters\",\"bro\",\"sis\",\"brother\",\"brothers\",\"married\",\"unmarried\",\"parent\",\"parents\"))\n\nBecause I wanted a have clean and easily interpretable analysis, I removed words of less than 3 letters.\n\n#Keep words with at least three letters\ntok&lt;-tokens_select(tok,min_nchar=3)\n\nNext, I create a “document-feature matrix” based on the counting of each “token” in each text. For instance, text 1 (family description of the first row in the sample database), corresponds to the following family description:\n“Our Family is very simple and having moderate values..I am retired Bank employees. My wife Kirtima Vaish is a homemaker and my two younger sons IT professionals . His youngest Brother in TCS San Francisco U S A. Second Youngest Brother coordinater in Sankalp India . Now we are living in Bangalore. He is currently living in Bangalore .We have two houses,one in Bangalore and another one in Faizabad.”\nAfter applying the corpus, token and pre-treatment, this amounts to counting for this text that family is a token mentioned once, simple is mentioned once, moderate once, etc… (the full matrix is not depicted here it has more columns and of course more rows).\n\n#Create document-feature matrix (keep in lower case)\ndtm &lt;- dfm(tok, tolower = TRUE)\n\nAgain, I now apply quite an arbitrary rule by retaining only the tokens that are mentioned at least ten times in all the corpus. I do this so that rarely frequently mentioned tokens do not structure heavily the output of the analysis.\n\n#Keep relatively frequent terms\ndtm &lt;- dfm_trim(dtm, min_docfreq = 10)"
  },
  {
    "objectID": "ExploratoryTextualAnalysis.html#textual-analysis",
    "href": "ExploratoryTextualAnalysis.html#textual-analysis",
    "title": "Exploratory textual analysis",
    "section": "Textual analysis",
    "text": "Textual analysis\n\nA word cloud as a first naive exploration\nAfter all this hard work, let’s conduct our first exploratory analysis! First, let’s just plot a word cloud where the size of words is proportional to their frequency in the corpus.\n\n#First a word cloud\ntextplot_wordcloud(dtm, random_order = F, rotation = 0.25,min_size =1,max_words = 100,\n                   color = RColorBrewer::brewer.pal(8, \"Dark2\"))\n\n\n\n\nUnsurprisingly, family is the most frequent term! Followed by younger, elder, probably characterizing “younger brother/sister”.\nThe other words all relate to the economic position of the household, either in terms of activity status (retired, housewife, working), occupation (engineer, business), economic sector (private/government sector) or social class (middle class). The other frequent words but relatively less (in green) also indicate the same kind of words to describe one’s family, with the addition that location also seems quite salient (Delhi, Bangalore…).\nThe word cloud is thus not only a nice plot, it is also informative on what are the most frequent tokens used, possibly indicating the most frequent topic in these descriptions. Here, it seems that when describing their family, spouse-to-be talk about the family economic position (and not about say, the hobbies of the family members).\n\n\nReinert’s method: clustering family descriptions\nNext, we turn to a more complex statistical tool to analyse the diversity of the family descriptions. Based on the “document-feature matrix,” we use the chi-square distance to calculate the similarities and differences between family descriptions. The method is based on an algorithm of Divisive Hierarchical Clustering (DHC), which can be described as such:\n\nWe split the documents into two groups, such that the two groups are as “different” as possible.\nThis difference is achieved by the fact that tokens that are frequently mentioned in one group are not frequently mentioned in the other group and vice versa.\nFrom the two groups of documents, the largest group is again split up into two groups.\nFrom the three groups of documents, the largest group is again split up into three groups.\n\nEtc…\nA more complete and precise description of the algorithm can be found here.\nThis step is achieved by running the following R command (here I ask the algorithm to split the documents into a maximum of six groups):\n\nres &lt;- rainette(dtm, k = 6)\n\nWarning in rainette(dtm, k = 6): some documents don't have any term, they won't\nbe assigned to any cluster.\n\n\n  Clustering...\n\n\n  Done.\n\n\nHow many groups or clusters of documents should we keep in the final partition?\nThis is an exploratory tool so the best way is to first analyse the two groups and interpret how they are different from each other, then the three groups, etc… At each step, we analyse what distinction emerges with the new cluster.\nAs a rule of thumb:\n\nKeep as many groups as you can interpret them\nRemember that your goal is to be able able to interpret distinctions in your corpus so it is not ideal to have too many clusters! Personally, I find that beyond 8-10 clusters, classifications are not informative as it is hard to grasp at a glance the full distinctions.\n\nLet’s now have a look at the clusters of documents and how they are structured by distinct tokens. This is done by a very nice shiny app:\n\nrainette_explor(res, dtm=dtm, corpus_src=corpus)\n\nA new window pops up. Let’s set the cursor to two clusters. The DHC has cut the family descriptions into a cluster gathering 82 percent of the descriptions and another one corresponding to 18 percent of the descriptions (notice that 15 clusters could not be assigned to any cluster because after pre-treating the texts, they did not contain any relevant term).\nEach cluster is characterized by its most specific tokens, i.e. broadly said the tokens that are often mentioned in the first cluster but not in the second cluster.\n\nThe first cluster corresponds to the descriptions that mention “family”, while this term is not frequently mentioned in the second cluster… The second one corresponds to descriptions that frequently mention “one”… These are not very informative tokens. One could think of re-running the analysis by adding these terms to the stop word list.\nThe first cluster is characterized by the phrases “middle class”, “simple values”, “nuclear family”, “manager”, “homemaker”…\nThe second one is characterized by the terms “housewife”, “farmer”, “private sector”, “business”…\n\n\nAs it may be difficult to interpret the specificity of each cluster, it may be good to come back to the descriptions and so how the most specific terms are used in the sentences. On the bottom right of the shiny app panel, you can click on “Cluster documents:”\n\nHere we look at the first cluster and how the token “middle” is used. It pops up with the phrase “middle class”. Matrimonial ads of the first cluster specifically describe their family as being a “middle class” one.\nWe can now turn to analyse three clusters, four clusters etc… I here stop at four clusters which I find informative and rather distinct from each other. It is nice to try to label them to summarize the interpretation.\n\nThe first cluster corresponds to the clusters of “Family composition and values” (32 percent of the descriptions)\nThe second cluster seems to be mostly about studies high professional achievements: “Family members’ educational curriculum and professional achievements” (31 percent)\nThe third cluster frequently has mentions of the army, the government sector and the mother/wife is often described as a “housewife”, we can label it the “Male members’ position in the army/govt sector and housewife female members” (20 percent)\nThe fourth cluster again corresponds to a frequent use of the token “housewife” and the work positions correspond more to private ones (business, farmer): “Male members’ private sector position and housewife female members” (18 percent)\n\n\nAll in all, all these types of family descriptions suggest that family is mostly described in terms of the socioeconomic achievements of the family members. The first cluster comes as an exception as the cluster insists more on family values, but even the family’s socioeconomic position is described as a whole with the very frequently used term “middle class”.\nDo these four types of family descriptions correspond to different characteristics of the matrimonial advertisements? For instance, are profiles managed by family members rather than the spouse-to-be associated to certain types of family descriptions?\nTo investigate this question, we save the clusters to our sample database as a new variable which we are going to open in the radiant app.\n\n###Decide to keep the partition in 4 classes\nsample$cluster &lt;- cutree(res, k = 4)\n\n\nsave(sample,file=\"sample2.rdata\")\n#Explore how these clusters correspond to specific profiles (male/female, ...)\nradiant::radiant_window()\n\nAfter loading the “sample2.rdata” file in the radiant app, we use the “Pivot” tab to investigate the associations between family description types and other matrimonial ad characteristics.\nAs can be seen from the pivot table and the plot, profiles run by the spouse-to-be themselves are slightly more often characterized by the first type of description (family composition and values). This is also the case of “others” who correspond to marriage brokers: they seem to reproduce a form of non-specific and general type of family description that corresponds to the ideal of a family.\nThe second (and also to a certain extant, the third) type of family descriptions (educational and professional achievements) is more characteristic of family members (parents and siblings). Insisting on the high educational and professional achievements is more salient to these members (than to the spouse-to-be themselves) who may be seeing marriage as a pure assortative matching process in terms of socioeconomic sorting.\nThe fourth type is a more spouse-to-be specific family description type.\n\nYou can explore other salient distinctions.\n\n\nCompound tokens?\nOne thing we have noticed in the course of this analysis is that some words come as single and separate tokens whereas they come as phrases, such as middle class, government sector, etc. We can try and detect these compound tokens and treat them as single terms in the analysis.\n\ntok &lt;- tokens(corpus)\n\ncol &lt;- tok |&gt; \n  tokens_remove(c(stopwords(\"en\"),\"mother\",\"father\",\"sister\",\"sisters\",\"mom\",\"dad\",\"bro\",\"sis\",\"brother\",\"brothers\",\"married\",\"unmarried\",\"parent\",\"parents\")) |&gt; \n  tokens_select(pattern = \"^[A-Z]\", valuetype = \"regex\", \n                case_insensitive = T, padding = TRUE) |&gt; \n  quanteda.textstats::textstat_collocations(min_count = 5, tolower = T)\nhead(col)\n\n      collocation count count_nested length   lambda        z\n1      house wife    88            0      2 6.085893 29.45430\n2  private sector    58            0      2 6.526536 25.53734\n3    middle class    83            0      2 9.192984 22.43732\n4      home maker    54            0      2 7.807280 22.32270\n5    well settled    44            0      2 4.511104 21.74900\n6 working private    57            0      2 4.561429 21.12930\n\ncomp_toks2 &lt;- tokens_compound(tok, pattern = col)\ncomp_toks2&lt;-tokens(comp_toks2,remove_punct = T,remove_symbols=TRUE,\n                   remove_numbers =TRUE)\ncomp_toks2 &lt;- tokens_remove(comp_toks2,c(stopwords(\"en\"),\"mother\",\"father\",\"sister\",\"sisters\",\"mom\",\"dad\",\"bro\",\"sis\",\"brother\",\"brothers\",\"married\",\"unmarried\",\"parent\",\"parents\"))\ncomp_toks2&lt;-tokens_select(comp_toks2,min_nchar=3)\n\ndtm &lt;- dfm(comp_toks2, tolower = TRUE)\ndtm &lt;- dfm_trim(dtm, min_docfreq = 10)\n\ntextplot_wordcloud(dtm, random_order = FALSE, rotation = 0.25,min_size =1,max_words = 100,\n                   color = RColorBrewer::brewer.pal(8, \"Dark2\"))\n\n\n\n\n\nres &lt;- rainette(dtm, k = 6)\n\nrainette_explor(res, dtm=dtm, corpus_src=corpus)"
  },
  {
    "objectID": "ExploratoryTextualAnalysis.html#textual-analysis-of-the-desired-partner",
    "href": "ExploratoryTextualAnalysis.html#textual-analysis-of-the-desired-partner",
    "title": "Exploratory textual analysis",
    "section": "Textual analysis of the desired partner",
    "text": "Textual analysis of the desired partner\nNext, let’s turn to the descriptions of the desired partner.\n\nThe “script” of the ideal partner\nWe use the same procedure and pre-treatment by running the code below. In the word cloud of the most frequent terms, do the adjectives look any family to gender scripts of the ideal desired matched partner?\n\ncorpus &lt;- corpus(sample$Desc_desiredpartner)\n\n#Remove punctuation & symbols\ntok &lt;- tokens(corpus, remove_punct = TRUE,\n              remove_symbols=TRUE,\n              remove_numbers =TRUE)\n\n#Remove stop words & words relating to specific family members\ntok &lt;- tokens_remove(tok,c(stopwords(\"en\")))\n#Keep words with at least three letters\ntok&lt;-tokens_select(tok,min_nchar=3)\n#Create document-feature matrix (keep in lower case)\ndtm &lt;- dfm(tok, tolower = TRUE)\n\n#Keep relatively frequent terms\ndtm &lt;- dfm_trim(dtm, min_docfreq = 10)\n\n#First a word cloud\ntextplot_wordcloud(dtm, random_order = FALSE, rotation = 0.25,min_size =1,max_words = 100,\n                   color = RColorBrewer::brewer.pal(8, \"Dark2\"))\n\n\n\n\nInterestingly, when classifying the desired partners, even with four clusters, we still have 84 percent of the desired partner descriptions that come in the same cluster. Notice how one cluster emerges because the words are not English terms but Hindi ones (transliterated in English).\n\nres &lt;- rainette(dtm, k = 6)\n\nrainette_explor(res, dtm=dtm, corpus_src=corpus)\n\n\n\n\nStratifying by gender\nInstead of trying to analyse whether these clusters correspond a posteriori to male or female scripts of the ideal partner, we may want to stratify a priori our sample between male and female profiles and see whether the aspirations of the desired partner are any different.\nWith the word cloud, try to interpret the gendered aspirations of the desired partner!\n\n# Textual analysis of the desired partner differentiating by gender---------------------------------------------------------\n#First, add the sex information to the corpus\ndocvars(corpus, \"Sex\") &lt;- sample$Sex\ntok2 &lt;- corpus %&gt;%\n  corpus_subset(Sex %in% c(\"Male\", \"Female\")) %&gt;%\n  tokens(remove_punct = TRUE,\n         remove_symbols=TRUE,\n         remove_numbers =TRUE) %&gt;%\n  tokens_remove(stopwords(\"en\")) %&gt;%\n  tokens_select(min_nchar=3)\ndfmat2 &lt;- dfm(tok2,tolower=T) %&gt;%\n  dfm_group(Sex) %&gt;%\ndfm_trim(min_termfreq = 3)\n\n\ntextplot_wordcloud(dfmat2, comparison = TRUE, min_size =1,max_words = 40,\n                   color = c(\"darkgreen\", \"darkorange\"))\n\n\n\n\nWe can also look at the list of the most typical tokens used by female and male profiles respectively:\n\n#The most typical tokens of female profiles regarding desired partner\nhead(tstat1 &lt;- textstat_keyness(dfmat2), 20)\n\n         feature     chi2            p n_target n_reference\n1         person 88.73288 0.000000e+00      115          62\n2            boy 60.00697 9.436896e-15       32           2\n3        settled 45.34756 1.649936e-11       31           6\n4            non 34.88173 3.503534e-09       23           4\n5     commitment 34.16181 5.071440e-09       18           1\n6           thin 27.86034 1.303954e-07       15           1\n7     compatible 26.59534 2.508482e-07       16           2\n8        dashing 26.59534 2.508482e-07       16           2\n9            guy 26.53958 2.581939e-07       14           0\n10         trust 24.54023 7.277429e-07       15           2\n11        worthy 24.43152 7.699868e-07       13           0\n12       amiable 21.64150 3.286631e-06       15           3\n13         human 21.46010 3.612676e-06       22           9\n14        honest 18.19730 1.991611e-05       57          55\n15        smoker 18.11712 2.077257e-05       10           0\n16 understanding 17.82373 2.423418e-05       79          88\n17    supporting 16.05262 6.160646e-05       12           2\n18       drinker 16.01698 6.277705e-05        9           0\n19           man 14.09676 1.736429e-04       11           2\n20       provide 12.80994 3.447829e-04        9           1\n\n#The most typical tokens of male profiles regarding desired partner\ntail(tstat1 &lt;- textstat_keyness(dfmat2), 20)\n\n             feature       chi2            p n_target n_reference\n591             life  -7.468835 6.277617e-03      117         329\n592          believe  -7.858283 5.058840e-03        8          47\n593       challenges  -8.070330 4.499591e-03        0          17\n594             term  -8.545806 3.463192e-03        0          18\n595           friend  -8.800662 3.011213e-03       10          56\n596           nature  -8.932972 2.800682e-03       16          75\n597           career  -9.574547 1.972935e-03        1          25\n598             soft  -9.841418 1.706259e-03        3          34\n599           spoken -10.513976 1.184751e-03        1          27\n600           simple -11.848578 5.770540e-04       31         128\n601          members -12.352622 4.403675e-04        0          26\n602       understand -13.057711 3.020383e-04       18          93\n603            ready -13.305163 2.646763e-04        0          28\n604           easily -13.570601 2.297558e-04        2          38\n605      comfortably -13.815948 2.016176e-04        1          34\n606 responsibilities -20.459909 6.089347e-06        0          43\n607       adjustable -20.937564 4.744981e-06        0          44\n608           family -37.486838 9.203195e-10       98         401\n609        beautiful -37.708634 8.214018e-10        0          79\n610             girl -96.464509 0.000000e+00        1         205\n\n\nFinally, we may want to classify the desired partner description of male and female profiles separately:\n\n##Exploratory analysis of men\ndfmat2male&lt;-tokens_subset(tok2, Sex==\"Male\") %&gt;% dfm(tolower=T) %&gt;%  dfm_trim(min_termfreq = 3)\ncorpusmale&lt;-corpus_subset(corpus,Sex==\"Male\")\nresmale &lt;- rainette(dfmat2male, k = 6)\n\nrainette_explor(resmale, dtm=dfmat2male, corpus_src=corpusmale)\n\n\nFor the female profiles:\n\n##Exploratory analysis of women\ndfmat2female&lt;-tokens_subset(tok2, Sex==\"Female\") %&gt;% dfm(tolower=T) %&gt;%  dfm_trim(min_termfreq = 3)\ncorpusfemale&lt;-corpus_subset(corpus,Sex==\"Female\")\nresfemale &lt;- rainette(dfmat2female, k = 6)\n\nrainette_explor(resfemale, dtm=dfmat2female, corpus_src=corpusfemale)\n\n\nWhat differences do you notice?\nWhat would you do if you wanted to analyse the descriptions according to who runs the matrimonial profiles?"
  }
]